# ðŸŽ¯ Job Ad Scraper - Scraper d'Emploi Intelligent

Un scraper d'emploi puissant avec **interface web** qui utilise **Google Search + Selenium** pour trouver des offres d'emploi **partout sur internet**, optimisÃ© pour les ingÃ©nieurs logiciels.

## ðŸš€ FonctionnalitÃ©s

- âœ… **Interface web intuitive** : ContrÃ´lez le scraping depuis votre navigateur
- âœ… **Base de donnÃ©es SQLite** : Stockage persistant et rapide des offres
- âœ… **Recherche Google intelligente** : Trouve les offres sur TOUS les sites d'emploi
- âœ… **Localisations multiples** : Suisse romande, Lille, tÃ©lÃ©travail
- âœ… **Score de compatibilitÃ©** : Calcule automatiquement la pertinence de chaque offre
- âœ… **Multi-sites** : Indeed, LinkedIn, Glassdoor, Welcome to the Jungle, jobs.ch
- âœ… **DÃ©duplication** : Supprime automatiquement les doublons
- âœ… **Suivi temps rÃ©el** : Progression du scraping en direct
- âœ… **Historique complet** : Sessions de scraping avec statistiques
- âœ… **Export multiple** : CSV, Excel depuis l'interface web

## ðŸš€ Installation

### 1. PrÃ©requis

- Python 3.8 ou supÃ©rieur
- pip (gestionnaire de paquets Python)

### 2. Installation des dÃ©pendances

```bash
# Installer les paquets Python requis
pip install -r requirements.txt
```

### 3. Configuration

1. Ouvrez le fichier `config.yaml`
2. Modifiez vos informations personnelles dans la section `user_profile`:
   - Nom et email
   - CompÃ©tences (sÃ©parÃ©es par des virgules)
   - AnnÃ©es d'expÃ©rience
   - Niveau d'Ã©ducation

3. Configurez vos critÃ¨res de recherche dans `search_criteria`:
   - Mots-clÃ©s pour le poste
   - Localisation prÃ©fÃ©rÃ©e
   - Fourchette salariale
   - Type de contrat
   - TÃ©lÃ©travail acceptÃ©

### 4. Installation de ChromeDriver (requis)

**macOS :**
```bash
brew install chromedriver
```

**Windows :**
- TÃ©lÃ©chargez depuis https://chromedriver.chromium.org/
- Ajoutez Ã  votre PATH

### 5. Lancement

#### Interface Web (RECOMMANDÃ‰E)
```bash
# Lancer l'interface web
./start_web.sh       # macOS/Linux
# ou
start_web.bat        # Windows

# Puis ouvrir http://localhost:8080
```

#### Mode Ligne de Commande
```bash
# Lancer directement le scraper
python main.py
```

## ðŸ“ Structure du projet

```
JobAdScrapper/
â”œâ”€â”€ ðŸŒ Interface Web
â”‚   â”œâ”€â”€ app.py               # Application Flask
â”‚   â”œâ”€â”€ templates/           # Templates HTML
â”‚   â”‚   â”œâ”€â”€ base.html
â”‚   â”‚   â”œâ”€â”€ index.html       # Tableau de bord
â”‚   â”‚   â”œâ”€â”€ jobs.html        # Liste des offres
â”‚   â”‚   â”œâ”€â”€ config.html      # Ã‰diteur de configuration
â”‚   â”‚   â””â”€â”€ sessions.html    # Historique
â”‚   â””â”€â”€ static/             # Ressources statiques
â”‚
â”œâ”€â”€ ðŸ”§ Scraper Core
â”‚   â”œâ”€â”€ main.py             # Moteur de scraping
â”‚   â””â”€â”€ config.yaml         # Configuration utilisateur
â”‚
â”œâ”€â”€ ðŸ—„ï¸ Base de donnÃ©es
â”‚   â”œâ”€â”€ jobs.db            # Base SQLite (crÃ©Ã©e automatiquement)
â”‚   â””â”€â”€ SQLITE_GUIDE.md    # Guide d'utilisation SQLite
â”‚
â”œâ”€â”€ ðŸš€ Scripts de lancement
â”‚   â”œâ”€â”€ start_web.sh       # Lancement macOS/Linux
â”‚   â””â”€â”€ start_web.bat      # Lancement Windows
â”‚
â””â”€â”€ ðŸ“‹ Documentation
    â”œâ”€â”€ README.md          # Ce fichier
    â””â”€â”€ requirements.txt   # DÃ©pendances Python
```

## âš™ï¸ Configuration dÃ©taillÃ©e

### Profil utilisateur (`user_profile`)

```yaml
user_profile:
  skills: "Python, JavaScript, React, Node.js, TypeScript, SQL, PostgreSQL, MongoDB, Docker, Kubernetes, AWS, GCP, Git, CI/CD"
  experience_years: 4
  education_level: "IngÃ©nieur"
```

### CritÃ¨res de recherche (`search_criteria`)

```yaml
search_criteria:
  keywords: 
    - "ingÃ©nieur logiciel"
    - "dÃ©veloppeur full stack"
    - "software engineer"
    - "full stack developer"
    - "tech lead"
    - "senior developer"
  
  locations:
    - "GenÃ¨ve, Suisse"
    - "Lausanne, Suisse"
    - "Vaud, Suisse"
    - "Lille, France"
    - "tÃ©lÃ©travail"
    - "full remote"
  
  salary_min: 40000
  salary_max: 1000000
  contract_types: ["CDI", "Freelance", "Mission longue"]
  remote_ok: true
```

### Sites de recherche (`job_sites`)

```yaml
job_sites:
  - name: "Indeed"
    url: "https://fr.indeed.com"
    enabled: true
  
  - name: "LinkedIn"
    url: "https://www.linkedin.com"
    enabled: false  # NÃ©cessite une authentification
```

## ðŸŽ¯ Score de compatibilitÃ©

Le systÃ¨me calcule automatiquement un score de compatibilitÃ© pour chaque offre :

- **40%** : Correspondance des compÃ©tences
- **30%** : AdÃ©quation salariale
- **20%** : Localisation
- **10%** : TÃ©lÃ©travail (si souhaitÃ©)

## ðŸŒ Interface Web

### Tableau de bord principal
- **Statistiques** : Total offres, score moyen, entreprises uniques
- **ContrÃ´le du scraping** : DÃ©marrage, progression temps rÃ©el
- **Offres rÃ©centes** : AperÃ§u des derniÃ¨res trouvailles
- **Historique** : Sessions prÃ©cÃ©dentes avec durÃ©e

### Pages disponibles
- **ðŸ  Accueil** : http://localhost:8080/
- **ðŸ’¼ Offres** : http://localhost:8080/jobs
- **âš™ï¸ Configuration** : http://localhost:8080/config
- **ðŸ“Š Sessions** : http://localhost:8080/sessions

### FonctionnalitÃ©s avancÃ©es
- **Filtrage** : Par score de compatibilitÃ©, entreprise, localisation
- **Pagination** : Navigation fluide dans les rÃ©sultats
- **Export** : CSV et Excel directement depuis l'interface
- **Ã‰dition configuration** : Ã‰diteur YAML intÃ©grÃ© avec validation
- **Progression temps rÃ©el** : Suivi en direct du scraping

## ðŸ—„ï¸ Base de donnÃ©es SQLite

### Stockage automatique
- **Fichier unique** : `jobs.db` contient toutes les donnÃ©es
- **DÃ©duplication automatique** : Plus de doublons
- **Historique complet** : Toutes les sessions sauvegardÃ©es
- **Recherche rapide** : Index optimisÃ©s

### Sauvegarde simple
```bash
# Copier le fichier = sauvegarde complÃ¨te
cp jobs.db backup_jobs_$(date +%Y%m%d).db
```

### Consultation des donnÃ©es
- **Interface web** : RecommandÃ©e pour la navigation
- **DB Browser** : `brew install --cask db-browser-for-sqlite`
- **Ligne de commande** : `sqlite3 jobs.db`

Pour plus de dÃ©tails, voir [SQLITE_GUIDE.md](SQLITE_GUIDE.md)

## ðŸ“Š Formats d'export

### Interface Web
- **CSV** : Export depuis la page des offres
- **Excel** : Avec formatage et filtres
- **API REST** : `/api/jobs` pour intÃ©grations

### Base de donnÃ©es
Toutes les offres sont stockÃ©es avec :
```sql
- title (titre du poste)
- company (entreprise)
- location (localisation)
- salary (salaire)
- description (description complÃ¨te)
- url (lien original)
- source (site source)
- match_score (score de compatibilitÃ©)
- scraped_at (date de dÃ©couverte)
```

## ðŸ”§ Personnalisation

### Via l'interface web
1. **AccÃ©dez** Ã  http://localhost:8080/config
2. **Modifiez** la configuration YAML en direct
3. **Sauvegardez** avec validation automatique
4. **Relancez** le scraping avec les nouveaux paramÃ¨tres

### ParamÃ¨tres du scraper v2
```yaml
scraper_settings:
  max_google_queries: 15        # RequÃªtes Google maximum
  max_results_per_query: 30     # RÃ©sultats par requÃªte
  max_jobs_total: 150          # Offres maximum Ã  traiter
  delay_between_queries: 8      # DÃ©lai entre requÃªtes (sec)
  headless: false              # Mode navigation visible
  random_delays: true          # DÃ©lais alÃ©atoires
  rotate_user_agents: true     # Rotation User-Agents
```

### Modifier le calcul du score
Modifiez la mÃ©thode `calculate_match_score()` dans `main.py` pour ajuster les critÃ¨res et leur pondÃ©ration.

## ðŸš¨ ConsidÃ©rations Ã©thiques

- **Respectez les conditions d'utilisation** des sites web
- **Utilisez des dÃ©lais appropriÃ©s** entre requÃªtes (configurÃ© Ã  2 secondes par dÃ©faut)
- **Ne surchargez pas les serveurs** avec trop de requÃªtes simultanÃ©es
- **Respectez les robots.txt** des sites

## ðŸ› DÃ©pannage

### Interface web ne se lance pas
```bash
# VÃ©rifier l'environnement virtuel
python3 -m venv job_scraper_env
source job_scraper_env/bin/activate  # macOS/Linux
# ou job_scraper_env\Scripts\activate  # Windows
pip install -r requirements.txt
```

### Port 8080 dÃ©jÃ  utilisÃ©
```bash
# Trouver le processus utilisant le port
lsof -i :8080
# ArrÃªter le processus ou changer de port dans app.py
```

### ChromeDriver non trouvÃ©
```bash
# macOS
brew install chromedriver

# VÃ©rifier l'installation
chromedriver --version
```

### Erreur de base de donnÃ©es
```bash
# VÃ©rifier l'intÃ©gritÃ©
sqlite3 jobs.db "PRAGMA integrity_check;"

# Sauvegarder et recrÃ©er si corruption
sqlite3 jobs.db ".dump" > backup.sql
rm jobs.db
sqlite3 jobs.db < backup.sql
```

### Pas de rÃ©sultats
- **VÃ©rifiez** la configuration dans l'interface web
- **Ã‰largissez** les mots-clÃ©s et localisations
- **Augmentez** `max_jobs_total` dans la configuration
- **Consultez** les logs dans le terminal

## ðŸ”® AmÃ©liorations futures

- âœ… ~~Interface web avec Flask~~ **FAIT !**
- âœ… ~~Base de donnÃ©es pour historique~~ **FAIT !**
- ðŸ”„ Notifications par email
- ðŸ”„ Support pour plus de sites (Glassdoor, jobs.ch)
- ðŸ”„ Filtres avancÃ©s dans l'interface
- ðŸ”„ Analyse de tendances et graphiques
- ðŸ”„ API REST complÃ¨te
- ðŸ”„ Mode sombre pour l'interface
- ðŸ”„ Alertes personnalisÃ©es

## ðŸ“ž Support

Pour toute question ou problÃ¨me :
1. **Consultez** cette documentation mise Ã  jour
2. **Lisez** le [Guide SQLite](SQLITE_GUIDE.md) pour la base de donnÃ©es
3. **VÃ©rifiez** les logs dans le terminal lors du scraping
4. **Testez** votre configuration dans l'interface web
5. **Consultez** l'historique des sessions pour diagnostiquer

### Ressources utiles
- **Interface web** : http://localhost:8080
- **Configuration** : http://localhost:8080/config
- **Guide SQLite** : [SQLITE_GUIDE.md](SQLITE_GUIDE.md)
- **Logs en temps rÃ©el** : Terminal lors du scraping

---

**Bon scraping avec votre nouvelle interface web ! ðŸŒðŸš€**