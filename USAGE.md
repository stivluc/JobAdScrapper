# ğŸš€ Guide d'utilisation - Job Ad Scraper

## ğŸ“‹ AperÃ§u

Ce scraper utilise **Google Search + Selenium** pour trouver des offres d'emploi **partout sur internet**. Il est optimisÃ© pour les ingÃ©nieurs logiciels recherchant des postes en **Suisse romande**, **Lille**, ou en **tÃ©lÃ©travail**.

## ğŸ¯ Configuration actuelle

### Profil ciblÃ©
- **Poste** : IngÃ©nieur logiciel / DÃ©veloppeur full stack
- **ExpÃ©rience** : 5 ans
- **CompÃ©tences** : Python, JavaScript, React, Node.js, Docker, AWS...

### Zones de recherche
1. **Suisse romande** : GenÃ¨ve, Lausanne, Vaud, Fribourg, NeuchÃ¢tel
2. **France** : Lille, MÃ©tropole Lilloise, Nord
3. **TÃ©lÃ©travail** : Offres 100% remote

### CritÃ¨res salariaux
- **Minimum** : 60 000â‚¬ annuels
- **Maximum** : 120 000â‚¬ annuels

## ğŸš€ Utilisation

### 1. Lancement rapide
```bash
# Activer l'environnement
source job_scraper_env/bin/activate

# Lancer le scraper
python3 main.py

# Ou utiliser le script interactif
./start_scraper.sh
```

### 2. Personnalisation
Ã‰ditez `config.yaml` pour adapter :
- **CompÃ©tences** : Ajoutez/supprimez des technologies
- **Localisations** : Modifiez les zones gÃ©ographiques
- **Salaires** : Ajustez les fourchettes
- **Mots-clÃ©s** : Changez les types de postes recherchÃ©s

### 3. RÃ©sultats
Les rÃ©sultats sont sauvegardÃ©s dans `results/` :
- **JSON** : DonnÃ©es complÃ¨tes pour analyse
- **CSV** : Import dans Excel/Google Sheets
- **Excel** : Format prÃªt pour filtrage

## ğŸ“Š Fonctionnement

### Phase 1 : Recherche Google
- GÃ©nÃ¨re ~15 requÃªtes de recherche optimisÃ©es
- Utilise Selenium pour simuler un navigateur
- Respecte les dÃ©lais anti-dÃ©tection

### Phase 2 : Scraping multi-sites
- Visite chaque URL d'offre trouvÃ©e
- Extrait : titre, entreprise, lieu, salaire, description
- Supporte : Indeed, LinkedIn, Glassdoor, jobs.ch

### Phase 3 : DÃ©duplication
- Supprime automatiquement les doublons
- Utilise titre + entreprise + lieu comme clÃ©

### Phase 4 : Scoring
- **40%** : Correspondance des compÃ©tences
- **30%** : AdÃ©quation salariale
- **20%** : Localisation (avec prioritÃ©)
- **10%** : TÃ©lÃ©travail

## ğŸ¯ Conseils d'optimisation

### Maximiser les rÃ©sultats
1. **Lancez le scraper le matin** (moins de trafic)
2. **Attendez 24h** entre deux sessions
3. **VÃ©rifiez config.yaml** avant chaque lancement
4. **Analysez les scores** : focalisez sur 70%+

### Ã‰viter les blocages
1. **Respectez les dÃ©lais** configurÃ©s
2. **N'exÃ©cutez pas trop frÃ©quemment**
3. **Surveillez les performances** systÃ¨me
4. **ArrÃªtez si dÃ©tection** (erreurs rÃ©pÃ©tÃ©es)

## ğŸ”§ Maintenance

### Mise Ã  jour des dÃ©pendances
```bash
source job_scraper_env/bin/activate
pip install --upgrade -r requirements.txt
```

### Mise Ã  jour ChromeDriver
```bash
brew upgrade chromedriver
```

### Ajout de nouveaux sites
1. Ajouter le domaine dans `is_job_url()`
2. CrÃ©er une mÃ©thode `scrape_nouveau_site()`
3. Ajouter le routage dans `scrape_job_url()`

## ğŸ“ˆ Performances typiques

### RÃ©sultats attendus
- **DurÃ©e** : 30-45 minutes
- **Offres trouvÃ©es** : 100-200
- **Offres uniques** : 80-150
- **Score moyen** : 60-80%

### Ressources systÃ¨me
- **CPU** : 30-50% pendant l'exÃ©cution
- **RAM** : 300-600MB
- **RÃ©seau** : 1-2 requÃªtes/seconde

## ğŸš¨ DÃ©pannage

### ProblÃ¨mes frÃ©quents

#### ChromeDriver non trouvÃ©
```bash
brew install chromedriver
```

#### Timeout Google
```yaml
# Dans config.yaml
scraper_settings:
  delay_between_queries: 12
  headless: true
```

#### Pas de rÃ©sultats
```yaml
# Ã‰largir les critÃ¨res
search_criteria:
  salary_min: 50000
  locations: ["Suisse", "France", "tÃ©lÃ©travail"]
```

## ğŸ‰ Bonnes pratiques

1. **VÃ©rifiez la configuration** avant chaque lancement
2. **Analysez les rÃ©sultats** pour affiner les critÃ¨res
3. **Sauvegardez les bons rÃ©sultats** avant modification
4. **Respectez les conditions** d'utilisation des sites
5. **Utilisez les donnÃ©es** de maniÃ¨re responsable

---

**ğŸš€ PrÃªt Ã  trouver votre emploi idÃ©al ? Lancez `python3 main.py` !**