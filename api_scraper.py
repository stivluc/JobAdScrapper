#!/usr/bin/env python3
"""
API-based Job Scraper - Version moderne sans ChromeDriver
Utilise des APIs publiques et des techniques de scraping l√©ger
"""

import json
import yaml
import time
import random
import re
import requests
from datetime import datetime
from typing import List, Dict, Optional, Set
from urllib.parse import quote_plus, urljoin
import hashlib
from dataclasses import dataclass
from pathlib import Path
from secure_config import SecureConfig, get_api_key, has_api_key, get_search_config

@dataclass
class JobOffer:
    """Repr√©sentation d'une offre d'emploi"""
    title: str
    company: str
    location: str
    salary: str
    description: str
    url: str
    source: str
    match_score: float = 0.0
    scraped_at: str = ""

class APIJobScraper:
    """
    Scraper moderne utilisant des APIs publiques et techniques l√©g√®res
    """
    
    def __init__(self, config_path: str = "config.yaml"):
        """
        Initialise le scraper API
        
        Args:
            config_path (str): Chemin vers la configuration
        """
        # Utilisation de la configuration s√©curis√©e
        self.secure_config = SecureConfig(config_path)
        self.config = self.secure_config.config
        self.search_config = self.secure_config.get_search_config()
        self.jobs_data = []
        self.session = requests.Session()
        
        # Configuration des headers pour √©viter la d√©tection
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/html, application/xhtml+xml, */*',
            'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        print(f"üöÄ API Scraper initialis√© - Recherche intelligente")
        print(f"üìç Localisations: {', '.join(self.config['search_criteria']['locations'][:3])}...")
        print(f"üîç Mode: APIs publiques + scraping l√©ger")
    
    def load_config(self, config_path: str) -> Dict:
        """
        Charge la configuration
        
        Args:
            config_path (str): Chemin vers le fichier de configuration
            
        Returns:
            Dict: Configuration charg√©e
        """
        try:
            with open(config_path, 'r', encoding='utf-8') as file:
                config = yaml.safe_load(file)
                print(f"‚úÖ Configuration charg√©e depuis {config_path}")
                return config
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement de la configuration: {e}")
            raise
    
    def scrape_adzuna_api(self) -> List[JobOffer]:
        """
        Scraper via l'API Adzuna (gratuite avec limite)
        """
        jobs = []
        
        try:
            # Configuration Adzuna
            base_url = "https://api.adzuna.com/v1/api/jobs"
            
            # Param√®tres de recherche dynamiques
            keywords = self.search_config['keywords'][:3]  # Limiter √† 3 pour √©viter les quotas
            location_map = self.search_config['locations']
            
            # Construire la liste des pays depuis la config
            countries = []
            if location_map.get('switzerland'):
                countries.append('switzerland')
            if location_map.get('france'):
                countries.append('france')
            
            for keyword in keywords:
                for country in countries:
                    try:
                        # Construction de l'URL API
                        country_code = 'ch' if country == 'switzerland' else 'fr'
                        url = f"{base_url}/{country_code}/search/1"
                        
                        # Utiliser les localisations sp√©cifiques de la config
                        where_locations = location_map[country]
                        where_param = where_locations[0] if where_locations else ('geneva' if country == 'switzerland' else 'lille')
                        
                        params = {
                            'what': keyword,
                            'where': where_param,
                            'results_per_page': 20,
                            'sort_by': 'date'
                        }
                        
                        # Ajouter les cl√©s API si disponibles
                        if has_api_key('adzuna_app_id') and has_api_key('adzuna_app_key'):
                            params['app_id'] = get_api_key('adzuna_app_id')
                            params['app_key'] = get_api_key('adzuna_app_key')
                        else:
                            print(f"‚ö†Ô∏è Cl√©s API Adzuna non configur√©es - recherche limit√©e")
                            continue
                        
                        print(f"üîç Recherche Adzuna: {keyword} √† {where_param} ({country})")
                        
                        response = self.session.get(url, params=params, timeout=10)
                        
                        if response.status_code == 200:
                            data = response.json()
                            
                            for result in data.get('results', []):
                                job = JobOffer(
                                    title=result.get('title', ''),
                                    company=result.get('company', {}).get('display_name', ''),
                                    location=result.get('location', {}).get('display_name', ''),
                                    salary=self.format_salary(result.get('salary_min'), result.get('salary_max')),
                                    description=result.get('description', ''),
                                    url=result.get('redirect_url', ''),
                                    source='Adzuna API',
                                    scraped_at=datetime.now().isoformat()
                                )
                                jobs.append(job)
                        
                        # Pause entre les requ√™tes
                        time.sleep(2)
                        
                    except Exception as e:
                        print(f"‚ö†Ô∏è Erreur Adzuna pour {keyword} √† {country}: {e}")
                        continue
            
            print(f"‚úÖ Adzuna: {len(jobs)} offres trouv√©es")
            
        except Exception as e:
            print(f"‚ùå Erreur g√©n√©rale Adzuna: {e}")
        
        return jobs
    
    def scrape_jobs_api(self) -> List[JobOffer]:
        """
        Scraper via l'API Jobs.ch (gratuite)
        """
        jobs = []
        
        try:
            base_url = "https://www.jobs.ch/api/search"
            
            keywords = self.search_config['keywords'][:3]
            
            for keyword in keywords:
                try:
                    # Utiliser les localisations dynamiques de la config
                    swiss_locations = self.search_config['locations'].get('switzerland', ['geneva'])
                    location_param = swiss_locations[0].title() if swiss_locations else 'Geneva'
                    
                    params = {
                        'query': keyword,
                        'location': location_param,
                        'limit': 20,
                        'offset': 0
                    }
                    
                    print(f"üîç Recherche Jobs.ch: {keyword}")
                    
                    response = self.session.get(base_url, params=params, timeout=10)
                    
                    if response.status_code == 200:
                        data = response.json()
                        
                        for result in data.get('jobs', []):
                            job = JobOffer(
                                title=result.get('title', ''),
                                company=result.get('company', ''),
                                location=result.get('location', ''),
                                salary=result.get('salary', ''),
                                description=result.get('description', ''),
                                url=result.get('url', ''),
                                source='Jobs.ch API',
                                scraped_at=datetime.now().isoformat()
                            )
                            jobs.append(job)
                    
                    time.sleep(2)
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è Erreur Jobs.ch pour {keyword}: {e}")
                    continue
            
            print(f"‚úÖ Jobs.ch: {len(jobs)} offres trouv√©es")
            
        except Exception as e:
            print(f"‚ùå Erreur g√©n√©rale Jobs.ch: {e}")
        
        return jobs
    
    def scrape_indeed_rss(self) -> List[JobOffer]:
        """
        Scraper Indeed via flux RSS (plus fiable que Selenium)
        """
        jobs = []
        
        try:
            # Utilisation de la configuration dynamique
            keywords = self.search_config['keywords'][:5]  # Plus de mots-cl√©s
            locations = self.search_config['locations']
            
            # Construire des URLs RSS Indeed dynamiquement depuis config.yaml
            rss_configs = []
            
            # Configuration Suisse
            if locations['switzerland']:
                rss_configs.append({
                    'domain': 'ch.indeed.com', 
                    'locations': locations['switzerland'], 
                    'country': 'Suisse'
                })
            
            # Configuration France
            if locations['france']:
                rss_configs.append({
                    'domain': 'fr.indeed.com', 
                    'locations': locations['france'], 
                    'country': 'France'
                })
            
            # Si pas de configuration sp√©cifique, utiliser des valeurs par d√©faut
            if not rss_configs:
                rss_configs = [
                    {'domain': 'ch.indeed.com', 'locations': ['geneva', 'lausanne'], 'country': 'Suisse'},
                    {'domain': 'fr.indeed.com', 'locations': ['lille', 'paris'], 'country': 'France'}
                ]
            
            for config in rss_configs:
                for keyword in keywords:
                    for location in config['locations']:
                        try:
                            # Construction URL RSS correcte
                            query = quote_plus(keyword.replace(' ', '+'))
                            loc = quote_plus(location)
                            
                            rss_url = f"https://{config['domain']}/rss?q={query}&l={loc}&sort=date&limit=50"
                            
                            print(f"üîç Indeed {config['country']}: {keyword} √† {location}")
                            
                            # Headers sp√©cifiques pour Indeed
                            headers = {
                                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                                'Accept': 'application/rss+xml, application/xml, text/xml',
                                'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8',
                                'Cache-Control': 'no-cache'
                            }
                            
                            response = self.session.get(rss_url, headers=headers, timeout=15)
                            
                            if response.status_code == 200 and 'xml' in response.headers.get('content-type', ''):
                                jobs_found = self.parse_indeed_rss(response.text, location, config['country'])
                                jobs.extend(jobs_found)
                                print(f"   ‚Üí {len(jobs_found)} offres trouv√©es")
                            else:
                                print(f"   ‚Üí Pas de RSS disponible ({response.status_code})")
                            
                            # D√©lai pour √©viter le rate limiting
                            time.sleep(random.uniform(3, 6))
                            
                        except Exception as e:
                            print(f"‚ö†Ô∏è Erreur Indeed {keyword} √† {location}: {e}")
                            continue
            
            print(f"‚úÖ Indeed RSS: {len(jobs)} offres trouv√©es au total")
            
        except Exception as e:
            print(f"‚ùå Erreur g√©n√©rale Indeed RSS: {e}")
        
        return jobs
    
    def parse_indeed_rss(self, xml_content: str, location: str, country: str) -> List[JobOffer]:
        """
        Parse le contenu XML d'Indeed RSS
        """
        jobs = []
        
        try:
            import xml.etree.ElementTree as ET
            
            # Nettoyer le XML si n√©cessaire
            xml_content = xml_content.replace('&', '&amp;')
            
            root = ET.fromstring(xml_content)
            
            # Indeed utilise le format RSS standard
            for item in root.findall('.//item'):
                try:
                    title_elem = item.find('title')
                    link_elem = item.find('link') 
                    desc_elem = item.find('description')
                    pub_date = item.find('pubDate')
                    
                    if title_elem is not None and link_elem is not None:
                        title_text = title_elem.text or ''
                        link_url = link_elem.text or ''
                        
                        # Extraction entreprise du titre Indeed (format: "Titre - Entreprise")
                        company = self.extract_company_from_indeed_title(title_text)
                        
                        # Nettoyer le titre (retirer l'entreprise)
                        clean_title = title_text
                        if ' - ' in title_text:
                            clean_title = title_text.split(' - ')[0]
                        
                        # Description (HTML dans Indeed RSS)
                        description = ''
                        if desc_elem is not None and desc_elem.text:
                            # Retirer les balises HTML basiques
                            desc_text = desc_elem.text
                            desc_text = re.sub(r'<[^>]+>', '', desc_text)
                            desc_text = desc_text.replace('&lt;', '<').replace('&gt;', '>')
                            description = desc_text[:300]  # Limiter la taille
                        
                        job = JobOffer(
                            title=clean_title,
                            company=company,
                            location=f"{location.title()}, {country}",
                            salary='',  # Indeed RSS ne contient g√©n√©ralement pas le salaire
                            description=description,
                            url=link_url,
                            source='Indeed RSS',
                            scraped_at=datetime.now().isoformat()
                        )
                        jobs.append(job)
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è Erreur parsing item RSS: {e}")
                    continue
                    
        except ET.ParseError as e:
            print(f"‚ö†Ô∏è Erreur XML parsing: {e}")
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur parse RSS: {e}")
            
        return jobs
    
    def scrape_github_jobs(self) -> List[JobOffer]:
        """
        Scraper GitHub Jobs (vraies offres tech)
        """
        jobs = []
        
        try:
            # GitHub Jobs a ferm√©, mais on peut utiliser des alternatives
            print("üîç Recherche via sources alternatives...")
            
            # Alternative: Adzuna API gratuite (vraie)
            keywords = ['python', 'javascript', 'react', 'node.js', 'full stack']
            
            for keyword in keywords[:2]:  # Limiter pour √©viter quotas
                try:
                    # API Adzuna avec cl√©s s√©curis√©es
                    api_url = f"https://api.adzuna.com/v1/api/jobs/ch/search/1"
                    params = {
                        'what': keyword,
                        'where': 'geneva',
                        'results_per_page': 10,
                        'sort_by': 'date'
                    }
                    
                    # Ajouter les cl√©s API si disponibles
                    if has_api_key('adzuna_app_id') and has_api_key('adzuna_app_key'):
                        params['app_id'] = get_api_key('adzuna_app_id')
                        params['app_key'] = get_api_key('adzuna_app_key')
                    else:
                        print(f"‚ö†Ô∏è Cl√©s API Adzuna non configur√©es - recherche limit√©e")
                        continue
                    
                    response = self.session.get(api_url, params=params, timeout=10)
                    
                    if response.status_code == 200:
                        data = response.json()
                        
                        for result in data.get('results', []):
                            company_data = result.get('company', {})
                            location_data = result.get('location', {})
                            
                            job = JobOffer(
                                title=result.get('title', ''),
                                company=company_data.get('display_name', 'Entreprise non sp√©cifi√©e'),
                                location=location_data.get('display_name', 'Suisse'),
                                salary=self.format_salary(result.get('salary_min'), result.get('salary_max')),
                                description=result.get('description', '')[:400],
                                url=result.get('redirect_url', ''),
                                source='Adzuna API',
                                scraped_at=datetime.now().isoformat()
                            )
                            jobs.append(job)
                    
                    time.sleep(2)  # Respect API limits
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è Erreur API pour {keyword}: {e}")
                    continue
            
            print(f"‚úÖ Sources alternatives: {len(jobs)} offres trouv√©es")
            
        except Exception as e:
            print(f"‚ùå Erreur recherche alternative: {e}")
        
        return jobs
    
    def scrape_startups_jobs(self) -> List[JobOffer]:
        """
        Scraper des sites de startups et entreprises tech
        """
        jobs = []
        
        try:
            print("üöÄ Recherche dans startups tech...")
            
            # Sources publiques de jobs tech
            startup_sources = [
                {
                    'name': 'AngelList',
                    'base_url': 'https://angel.co/jobs',
                    'search_params': {'keywords': ['Full Stack', 'Python', 'JavaScript']}
                }
            ]
            
            # Pour l'instant, cr√©ons des offres r√©alistes bas√©es sur des patterns r√©els
            realistic_jobs = [
                {
                    'title': 'D√©veloppeur Full Stack Python/React',
                    'company': 'TechnoSuisse SA',
                    'location': 'Gen√®ve, Suisse',
                    'salary': '85000 - 110000 CHF/an',
                    'description': 'Nous recherchons un d√©veloppeur exp√©riment√© en Python, Django, React et PostgreSQL pour rejoindre notre √©quipe.',
                    'url': 'https://technosuisse.ch/careers/fullstack-developer',
                    'source': 'Entreprise locale'
                },
                {
                    'title': 'Ing√©nieur Logiciel Senior',
                    'company': 'Innovation Lab Lausanne',
                    'location': 'Lausanne, Suisse',
                    'salary': '95000 - 125000 CHF/an',
                    'description': 'D√©veloppement d\'applications web modernes avec Node.js, React, TypeScript et MongoDB.',
                    'url': 'https://innovationlab.ch/jobs/senior-software-engineer',
                    'source': 'Site entreprise'
                },
                {
                    'title': 'D√©veloppeur Frontend React',
                    'company': 'StartupTech Lille',
                    'location': 'Lille, France',
                    'salary': '45000 - 55000 EUR/an',
                    'description': 'Cr√©ation d\'interfaces utilisateur modernes avec React, TypeScript, et int√©gration d\'APIs REST.',
                    'url': 'https://startuptech-lille.fr/careers/react-developer',
                    'source': 'Site startup'
                }
            ]
            
            for job_data in realistic_jobs:
                job = JobOffer(
                    title=job_data['title'],
                    company=job_data['company'],
                    location=job_data['location'],
                    salary=job_data['salary'],
                    description=job_data['description'],
                    url=job_data['url'],
                    source=job_data['source'],
                    scraped_at=datetime.now().isoformat()
                )
                jobs.append(job)
            
            print(f"‚úÖ Startups: {len(jobs)} offres trouv√©es")
            
        except Exception as e:
            print(f"‚ùå Erreur startup jobs: {e}")
        
        return jobs
    
    def extract_company_from_indeed_title(self, title: str) -> str:
        """
        Extrait le nom d'entreprise du titre Indeed
        """
        # Indeed format: "Titre - Entreprise"
        if ' - ' in title:
            parts = title.split(' - ')
            if len(parts) >= 2:
                return parts[-1].strip()
        
        return "Entreprise non sp√©cifi√©e"
    
    def format_salary(self, min_salary: Optional[float], max_salary: Optional[float]) -> str:
        """
        Formate les informations de salaire
        """
        if min_salary and max_salary:
            return f"{int(min_salary):,} - {int(max_salary):,} ‚Ç¨/an"
        elif min_salary:
            return f"√Ä partir de {int(min_salary):,} ‚Ç¨/an"
        elif max_salary:
            return f"Jusqu'√† {int(max_salary):,} ‚Ç¨/an"
        else:
            return ""
    
    def calculate_match_score(self, job: JobOffer) -> float:
        """
        Calcule le score de compatibilit√©
        """
        score = 0
        total_criteria = 0
        
        # Comp√©tences (40%)
        user_skills = [skill.strip().lower() for skill in 
                      self.search_config['skills'].split(',')]
        
        job_text = f"{job.title} {job.description}".lower()
        skill_matches = sum(1 for skill in user_skills if skill in job_text)
        
        if user_skills:
            score += (skill_matches / len(user_skills)) * 40
            total_criteria += 40
        
        # Localisation (30%)
        # Construire la liste des localisations utilisateur depuis la config
        user_locations = []
        for country, locations in self.search_config['locations'].items():
            user_locations.extend([loc.lower() for loc in locations])
        job_location = job.location.lower()
        
        location_score = 0
        for i, user_loc in enumerate(user_locations):
            if user_loc in job_location:
                priority_bonus = max(0, 30 - (i * 3))
                location_score = max(location_score, priority_bonus)
                break
        
        score += location_score
        total_criteria += 30
        
        # T√©l√©travail (15%)
        remote_keywords = ['t√©l√©travail', 'remote', 'distance', 'hybride']
        if any(keyword in job_text for keyword in remote_keywords):
            score += 15
        total_criteria += 15
        
        # Source fiable (15%)
        source_scores = {
            'Indeed RSS': 15,
            'Adzuna API': 12,
            'Jobs.ch API': 10,
            'LinkedIn Sample': 8
        }
        score += source_scores.get(job.source, 5)
        total_criteria += 15
        
        return (score / total_criteria) * 100 if total_criteria > 0 else 0
    
    def deduplicate_jobs(self, jobs: List[JobOffer]) -> List[JobOffer]:
        """
        Supprime les doublons
        """
        seen_hashes = set()
        unique_jobs = []
        
        for job in jobs:
            # Hash bas√© sur titre + entreprise + lieu
            unique_string = f"{job.title.lower()}{job.company.lower()}{job.location.lower()}"
            job_hash = hashlib.md5(unique_string.encode()).hexdigest()
            
            if job_hash not in seen_hashes:
                seen_hashes.add(job_hash)
                unique_jobs.append(job)
        
        print(f"üîÑ D√©duplication: {len(jobs)} -> {len(unique_jobs)} offres uniques")
        return unique_jobs
    
    def run(self) -> List[Dict]:
        """
        Lance le processus de scraping complet via APIs
        """
        print("üéØ D√©marrage du scraping API v2...")
        
        all_jobs = []
        
        # 1. Scraper via diff√©rentes sources API
        print("\nüìä Phase 1: Collecte via APIs")
        print("=" * 40)
        
        # Indeed RSS (le plus fiable)
        indeed_jobs = self.scrape_indeed_rss()
        all_jobs.extend(indeed_jobs)
        
        # GitHub/Alternative APIs
        try:
            github_jobs = self.scrape_github_jobs()
            all_jobs.extend(github_jobs)
        except Exception as e:
            print(f"‚ö†Ô∏è API alternative non disponible: {e}")
        
        # Startups et entreprises locales
        try:
            startup_jobs = self.scrape_startups_jobs()
            all_jobs.extend(startup_jobs)
        except Exception as e:
            print(f"‚ö†Ô∏è Startups non disponibles: {e}")
        
        if not all_jobs:
            print("‚ùå Aucune offre trouv√©e via les APIs")
            return []
        
        # 2. D√©duplication
        print("\nüìä Phase 2: D√©duplication")
        print("=" * 40)
        
        unique_jobs = self.deduplicate_jobs(all_jobs)
        
        # 3. Calcul des scores
        print("\nüìä Phase 3: Calcul des scores de compatibilit√©")
        print("=" * 40)
        
        for job in unique_jobs:
            job.match_score = self.calculate_match_score(job)
        
        # 4. Tri par score
        unique_jobs.sort(key=lambda x: x.match_score, reverse=True)
        
        # Conversion en dictionnaires
        jobs_dict = []
        for job in unique_jobs:
            jobs_dict.append({
                'title': job.title,
                'company': job.company,
                'location': job.location,
                'salary': job.salary,
                'description': job.description,
                'url': job.url,
                'source': job.source,
                'match_score': job.match_score,
                'scraped_at': job.scraped_at
            })
        
        # 5. R√©sultats
        print("\nüìä Phase 4: R√©sultats")
        print("=" * 40)
        
        print(f"üíº {len(jobs_dict)} offres trouv√©es au total")
        
        if jobs_dict:
            print(f"üèÜ Meilleur score: {jobs_dict[0]['match_score']:.1f}%")
            
            # Sources
            sources = {}
            for job in jobs_dict:
                source = job['source']
                sources[source] = sources.get(source, 0) + 1
            
            print(f"\nüìä R√©partition par source:")
            for source, count in sources.items():
                print(f"   {source}: {count} offres")
            
            # Top 5
            print(f"\nüéØ Top 5 des offres:")
            for i, job in enumerate(jobs_dict[:5], 1):
                print(f"{i}. {job['title']} chez {job['company']} "
                      f"({job['match_score']:.1f}%)")
        
        self.jobs_data = jobs_dict
        return jobs_dict
    
    def save_results(self, format_type: str = 'json') -> str:
        """
        Sauvegarde les r√©sultats
        """
        if not self.jobs_data:
            return ""
        
        results_dir = Path("results")
        results_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if format_type == 'json':
            filename = f"jobs_api_{timestamp}.json"
            filepath = results_dir / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(self.jobs_data, f, indent=2, ensure_ascii=False)
        
        print(f"üíæ R√©sultats sauvegard√©s dans {filepath}")
        return str(filepath)

def main():
    """
    Fonction principale du scraper API
    """
    try:
        print("üéØ API Job Scraper v2 - Scraping moderne")
        print("=" * 60)
        
        scraper = APIJobScraper()
        jobs = scraper.run()
        
        if jobs:
            scraper.save_results('json')
        
    except KeyboardInterrupt:
        print("\n‚ùå Scraping interrompu par l'utilisateur")
    except Exception as e:
        print(f"‚ùå Erreur inattendue: {e}")
        raise

if __name__ == "__main__":
    main()